{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Hands-on Tutorial: Measuring Unintended Bias in Text Classification Models with Real Data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Usage Instructions\n",
    "\n",
    "This notebook can be run as a Kaggle Kernel with no installation requirements.\n",
    "\n",
    "To run the notebook locally you should:\n",
    "\n",
    "* Install all Python dependencies from the `requirements.txt` file\n",
    "* Download all training, validation, and test files"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from __future__ import absolute_import\n",
    "from __future__ import division\n",
    "from __future__ import print_function\n",
    "\n",
    "import os\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import pkg_resources\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import time\n",
    "import scipy.stats as stats\n",
    "\n",
    "from keras.preprocessing.text import Tokenizer\n",
    "from keras.utils import to_categorical\n",
    "from keras.preprocessing.sequence import pad_sequences\n",
    "from keras.layers import Embedding\n",
    "from keras.layers import Input\n",
    "from keras.layers import Conv1D\n",
    "from keras.layers import MaxPooling1D\n",
    "from keras.layers import Flatten\n",
    "from keras.layers import Dropout\n",
    "from keras.layers import Dense\n",
    "from keras.optimizers import RMSprop\n",
    "from keras.models import Model\n",
    "\n",
    "%matplotlib inline\n",
    "\n",
    "# autoreload makes it easier to interactively work on code in imported libraries\n",
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load and pre-process data sets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# These files will be provided to tutorial participants via Google Cloud Storage\n",
    "train_v1_df = pd.read_csv('../input/fat-star-tutorial-data/public_train_v1.csv')\n",
    "validate_df = pd.read_csv('../input/fat-star-tutorial-data/public_validate.csv')\n",
    "test_df = pd.read_csv('../input/fat-star-tutorial-data/public_test.csv')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's examine some rows in these datasets.  Note that columns like toxicity and male are percent scores.\n",
    "We query for \"male >= 0\" to exclude rows where the male identity is not labeled."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_df[['toxicity', 'male', 'comment_text']].query('male >= 0').head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We will need to convert toxicity and identity columns to booleans, in order to work with our neural net and metrics calculcations.  For this tutorial, we will consider any value >= 0.5 as True (i.e. a comment should be considered toxic if 50% or more crowd raters labeled it as toxic).  Note that this code also converts missing identity fields to False."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# List all identities\n",
    "identity_columns = [\n",
    "    'male', 'female', 'transgender', 'other_gender', 'heterosexual', 'homosexual_gay_or_lesbian',\n",
    "    'bisexual', 'other_sexual_orientation', 'christian', 'jewish', 'muslim', 'hindu', 'buddhist',\n",
    "    'atheist', 'other_religion', 'black', 'white', 'asian', 'latino', 'other_race_or_ethnicity',\n",
    "    'physical_disability', 'intellectual_or_learning_disability', 'psychiatric_or_mental_illness', 'other_disability']\n",
    "\n",
    "def convert_to_bool(df, col_name):\n",
    "    df[col_name] = np.where(df[col_name] >= 0.5, True, False)\n",
    "\n",
    "for df in [train_v1_df, validate_df, test_df]:\n",
    "    for col in ['toxicity'] + identity_columns:\n",
    "        convert_to_bool(df, col)\n",
    "    \n",
    "train_v1_df[['toxicity', 'male', 'comment_text']].head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Create and Train Models\n",
    "\n",
    "This code creates and trains a convolutional neural net using the Keras framework.  This neural net accepts a text comment, encoding as a sequence of integers, and outputs a probably that the comment is toxic.  Don't worry if you do not understand all of this code, as we will be treating this neural net as a black box later in the tutorial."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "MAX_SEQUENCE_LENGTH = 250\n",
    "MAX_NUM_WORDS = 10000\n",
    "TOXICITY_COLUMN = 'toxicity'\n",
    "TEXT_COLUMN = 'comment_text'\n",
    "EMBEDDINGS_PATH = '../input/glove6b/glove.6B.100d.txt'\n",
    "EMBEDDINGS_DIMENSION = 100\n",
    "DROPOUT_RATE = 0.3\n",
    "LEARNING_RATE = 0.00005\n",
    "NUM_EPOCHS = 10\n",
    "BATCH_SIZE = 128"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def pad_text(texts, tokenizer):\n",
    "    return pad_sequences(tokenizer.texts_to_sequences(texts), maxlen=MAX_SEQUENCE_LENGTH)\n",
    "\n",
    "def train_model(train_df, validate_df, tokenizer):\n",
    "    # Prepare data\n",
    "    train_text = pad_text(train_df[TEXT_COLUMN], tokenizer)\n",
    "    train_labels = to_categorical(train_df[TOXICITY_COLUMN])\n",
    "    validate_text = pad_text(validate_df[TEXT_COLUMN], tokenizer)\n",
    "    validate_labels = to_categorical(validate_df[TOXICITY_COLUMN])\n",
    "\n",
    "    # Load embeddings\n",
    "    embeddings_index = {}\n",
    "    with open(EMBEDDINGS_PATH) as f:\n",
    "        for line in f:\n",
    "            values = line.split()\n",
    "            word = values[0]\n",
    "            coefs = np.asarray(values[1:], dtype='float32')\n",
    "            embeddings_index[word] = coefs\n",
    "\n",
    "    embedding_matrix = np.zeros((len(tokenizer.word_index) + 1,\n",
    "                                 EMBEDDINGS_DIMENSION))\n",
    "    num_words_in_embedding = 0\n",
    "    for word, i in tokenizer.word_index.items():\n",
    "        embedding_vector = embeddings_index.get(word)\n",
    "        if embedding_vector is not None:\n",
    "            num_words_in_embedding += 1\n",
    "            # words not found in embedding index will be all-zeros.\n",
    "            embedding_matrix[i] = embedding_vector\n",
    "\n",
    "    # Create model layers.\n",
    "    def get_convolutional_neural_net_layers():\n",
    "        \"\"\"Returns (input_layer, output_layer)\"\"\"\n",
    "        sequence_input = Input(shape=(MAX_SEQUENCE_LENGTH,), dtype='int32')\n",
    "        embedding_layer = Embedding(len(tokenizer.word_index) + 1,\n",
    "                                    EMBEDDINGS_DIMENSION,\n",
    "                                    weights=[embedding_matrix],\n",
    "                                    input_length=MAX_SEQUENCE_LENGTH,\n",
    "                                    trainable=False)\n",
    "        x = embedding_layer(sequence_input)\n",
    "        x = Conv1D(128, 5, activation='relu', padding='same')(x)\n",
    "        x = MaxPooling1D(5, padding='same')(x)\n",
    "        x = Conv1D(128, 5, activation='relu', padding='same')(x)\n",
    "        x = MaxPooling1D(5, padding='same')(x)\n",
    "        x = Conv1D(128, 5, activation='relu', padding='same')(x)\n",
    "        x = MaxPooling1D(40, padding='same')(x)\n",
    "        x = Flatten()(x)\n",
    "        x = Dropout(DROPOUT_RATE)(x)\n",
    "        x = Dense(128, activation='relu')(x)\n",
    "        preds = Dense(2, activation='softmax')(x)\n",
    "        return sequence_input, preds\n",
    "\n",
    "    # Compile model.\n",
    "    input_layer, output_layer = get_convolutional_neural_net_layers()\n",
    "    model = Model(input_layer, output_layer)\n",
    "    model.compile(loss='categorical_crossentropy',\n",
    "                  optimizer=RMSprop(lr=LEARNING_RATE),\n",
    "                  metrics=['acc'])\n",
    "\n",
    "    # Train model.\n",
    "    model.fit(train_text,\n",
    "              train_labels,\n",
    "              batch_size=BATCH_SIZE,\n",
    "              epochs=NUM_EPOCHS,\n",
    "              validation_data=(validate_text, validate_labels),\n",
    "              verbose=2)\n",
    "\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "MODEL_NAME_V1 = 'fat_star_tutorial_v1'\n",
    "tokenizer_v1 = Tokenizer(num_words=MAX_NUM_WORDS)\n",
    "tokenizer_v1.fit_on_texts(train_v1_df[TEXT_COLUMN])\n",
    "model_v1 = train_model(train_v1_df, validate_df, tokenizer_v1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Score test set with the new model\n",
    "\n",
    "Using our new model, we can score the set of test comments for toxicity."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_comments_padded = pad_text(test_df[TEXT_COLUMN], tokenizer_v1)\n",
    "test_df[MODEL_NAME_V1] = model_v1.predict(test_comments_padded)[:, 1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Print some records to compare our model resulsts with the correct labels\n",
    "test_df[[TOXICITY_COLUMN, TEXT_COLUMN, MODEL_NAME_V1]].head(10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Measure bias\n",
    "\n",
    "Using metrics based on Pinned AUC and the Mann Whitney U test, we can measure our model for biases against different identity groups.  We only calculate bias metrics on identities that are refered to in 100 or more comments, to minimize noise."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get a list of identity columns that have >= 100 True records.  This will remove groups such\n",
    "# as \"other_disability\" which do not have enough records to calculate meaningful metrics.\n",
    "identities_with_over_100_records = []\n",
    "for identity in identity_columns:\n",
    "    num_records = len(test_df.query(identity + '==True'))\n",
    "    if num_records >= 100:\n",
    "        identities_with_over_100_records.append(identity)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_normalized_pinned_auc(df, subgroup, model_name):\n",
    "    subgroup_non_toxic = df[df[subgroup] & ~df[TOXICITY_COLUMN]]\n",
    "    subgroup_toxic = df[df[subgroup] & df[TOXICITY_COLUMN]]\n",
    "    background_non_toxic = df[~df[subgroup] & ~df[TOXICITY_COLUMN]]\n",
    "    background_toxic = df[~df[subgroup] & df[TOXICITY_COLUMN]]\n",
    "    \n",
    "    within_subgroup_mwu = normalized_mwu(subgroup_non_toxic, subgroup_toxic, model_name)\n",
    "    cross_negative_mwu = normalized_mwu(subgroup_non_toxic, background_toxic, model_name)\n",
    "    cross_positive_mwu = normalized_mwu(background_non_toxic, subgroup_toxic, model_name)\n",
    "    \n",
    "    return np.mean([1 - within_subgroup_mwu, 1 - cross_negative_mwu, 1 - cross_positive_mwu])\n",
    "\n",
    "def normalized_mwu(data1, data2, model_name):\n",
    "    \"\"\"Returns the number of pairs where the datapoint in data1 has a greater score than that from data2.\"\"\" \n",
    "    scores_1 = data1[model_name]\n",
    "    scores_2 = data2[model_name]\n",
    "    n1 = len(scores_1)\n",
    "    n2 = len(scores_2)\n",
    "    u, _ = stats.mannwhitneyu(scores_1, scores_2, alternative = 'less')\n",
    "    return u/(n1*n2)\n",
    "\n",
    "def compute_pinned_auc(df, identity, model_name):\n",
    "    # Create combined_df, containing an equal number of comments that refer to the identity, and\n",
    "    # that belong to the background distribution.\n",
    "    identity_df = df[df[identity]]\n",
    "    nonidentity_df = df[~df[identity]].sample(len(identity_df), random_state=25)\n",
    "    combined_df = pd.concat([identity_df, nonidentity_df])\n",
    "\n",
    "    # Calculate the Pinned AUC\n",
    "    true_labels = combined_df[TOXICITY_COLUMN]\n",
    "    predicted_labels = combined_df[model_name]\n",
    "    return metrics.roc_auc_score(true_labels, predicted_labels)\n",
    "\n",
    "def get_bias_metrics(df, model_name):\n",
    "    bias_metrics_df = pd.DataFrame({\n",
    "        'subgroup': identities_with_over_100_records,\n",
    "        'pinned_auc': [compute_pinned_auc(df, identity, model_name)\n",
    "                       for identity in identities_with_over_100_records],\n",
    "        'normalized_pinned_auc': [compute_normalized_pinned_auc(df, identity, model_name)\n",
    "                                  for identity in identities_with_over_100_records]\n",
    "    })\n",
    "    # Re-order columns and sort bias metrics\n",
    "    return bias_metrics_df[['subgroup', 'pinned_auc', 'normalized_pinned_auc']].sort_values('pinned_auc')\n",
    "\n",
    "def calculate_overall_auc(df, model_name):\n",
    "    true_labels = df[TOXICITY_COLUMN]\n",
    "    predicted_labels = df[model_name]\n",
    "    return metrics.roc_auc_score(true_labels, predicted_labels)    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "bias_metrics_df = get_bias_metrics(test_df, MODEL_NAME_V1)\n",
    "bias_metrics_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "calculate_overall_auc(test_df, MODEL_NAME_V1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can graph a histogram of comment scores in each identity.  In the following graphs, the X axis represents the toxicity score given by our new model, and the Y axis represents the comment count.  Blue values are comment whose true label is non-toxic, while red values are those whose true label is toxic.\n",
    "\n",
    "We can see that for some identities such as Asian, the model scores most non-toxic comments as less than 0.2 and most toxic comments as greater than 0.2.  This indicates that for the Asian identity, our model is able to distinguish between toxic and non-toxic comments.  However, for the black identity, there are many non-toxic comments with scores over 0.5, along with many toxic comments with scores of less than 0.5.  This shows that for the black identity, our model will be less accurate at separating toxic comments from non-toxic comments."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot toxicity distributions of different identities to visualize bias.\n",
    "def plot_histogram(identity):\n",
    "    toxic_scores = test_df.query(identity + ' == True & toxicity == True')[MODEL_NAME_V1]\n",
    "    non_toxic_scores = test_df.query(identity + ' == True & toxicity == False')[MODEL_NAME_V1]\n",
    "    sns.distplot(non_toxic_scores, color=\"skyblue\", axlabel=identity)\n",
    "    sns.distplot(toxic_scores, color=\"red\", axlabel=identity)\n",
    "    plt.figure()\n",
    "\n",
    "for identity in bias_metrics_df['subgroup']:\n",
    "    plot_histogram(identity)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Train a new model on more data to reduce some bias\n",
    "\n",
    "While not the only source of bias, one possible explanation for our models poor performance on certain identities is a lack of properly balanced training data across identities. For example, if we query comments labeled as \"homosexual\\_gay\\_or\\_lesbian\" in our initial training data, we see that a larger proportion of those comments are rated toxic than in the entire dataset.  We can try to fix this imbalance and re-train our model using additional training data that contains more non-toxic comments labeled as \"homosexual\\_gay\\_or\\_lesbian\".  After re-training, we hope to improve the Pinned AUC score for this identity."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load new training data and convert fields to booleans.\n",
    "train_v2_df = pd.read_csv('../input/fat-star-tutorial-data/public_train_v2.csv')\n",
    "for col in ['toxicity'] + identity_columns:\n",
    "    convert_to_bool(train_v2_df, col)\n",
    "\n",
    "# Create a new model using the same structure as our model_v1.\n",
    "MODEL_NAME_V2 = 'fat_star_tutorial_v2'\n",
    "tokenizer_v2 = Tokenizer(num_words=MAX_NUM_WORDS)\n",
    "tokenizer_v2.fit_on_texts(train_v2_df[TEXT_COLUMN])\n",
    "model_v2 = train_model(train_v2_df, validate_df, tokenizer_v2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_comments_padded_v2 = pad_text(test_df[TEXT_COLUMN], tokenizer_v2)\n",
    "test_df[MODEL_NAME_V2] = model_v2.predict(test_comments_padded_v2)[:, 1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "bias_metrics_v2_df = get_bias_metrics(test_df, MODEL_NAME_V2)\n",
    "bias_metrics_v2_df"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
